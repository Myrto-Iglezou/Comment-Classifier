{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Mining - Project 3 - Spring Semester 2020\n",
    "---\n",
    "##  Iglezou Myrto  | sdi1700038\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import sys # only needed to determine Python version number\n",
    "import matplotlib # only needed to determine Matplotlib version number\n",
    "import nltk\n",
    "from pandas import DataFrame, read_csv\n",
    "from nltk import word_tokenize, BigramCollocationFinder\n",
    "from nltk.corpus import stopwords as stopwords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\train.csv\" \n",
    "testPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\impermium_verification_set.csv\" \n",
    "y_testPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\impermium_verification_labels.csv\"\n",
    "tempfile = pd.read_csv(trainPath,index_col=False)\n",
    "traindf = pd.DataFrame(data=tempfile)\n",
    "tempfile = pd.read_csv(testPath,index_col=False)\n",
    "testdf = pd.DataFrame(data=tempfile)\n",
    "tempfile = pd.read_csv(y_testPath,index_col=False)\n",
    "y_testdf = pd.DataFrame(data=tempfile)\n",
    "y_test = y_testdf[\"Insult\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from wordcloud import STOPWORDS,WordCloud\n",
    "\n",
    "punctuation = list(punctuation)\n",
    "punctuation.append('â€™')\n",
    "stopwordsPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\english_stopwords.txt\"\n",
    "file = open(stopwordsPath,\"r\",errors='ignore')\n",
    "stopwords_fromHub = [x.replace('\\n',\"\") for x in file.readlines()]\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords_nltk =  set(stopwords2.words('english'))\n",
    "stopwords.update(stopwords_nltk)\n",
    "stopwords.update(set(stopwords_fromHub))\n",
    "\n",
    "punctuation.append(\"``\")\n",
    "punctuation.append(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeData(X_train,X_test,vectorizer):\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def classifier(NB,x_train, y_train,x_test,y_test):\n",
    "    clf = NB\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    train_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    train_f1 = metrics.f1_score(y_test, y_pred)\n",
    "    print(\"Accuracy score = \",train_accuracy)\n",
    "    print(\"F1 score = \",train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "testdf['Comment'] = testdf['Comment'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "traindf['Comment'] = traindf['Comment'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "\n",
    "x_train = traindf['Comment']\n",
    "y_train = traindf['Insult']\n",
    "x_test = testdf['Comment']\n",
    "\n",
    "def cleanText(data):\n",
    "    processed_data = []   \n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        text = re.sub(\"[^a-zA-Z]\",\" \",text) # matches everything from numbers and punctuations etc , leaving only the words\n",
    "        for word in word_tokenize(text):\n",
    "            word = word.lower()\n",
    "            if (word not in punctuation) and word.isalpha():\n",
    "                textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))\n",
    "\n",
    "def lemmatize(data):\n",
    "    processed_data = []   \n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        for word in word_tokenize(text):\n",
    "            word = WordNetLemmatizer().lemmatize(word)\n",
    "            textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))\n",
    "\n",
    "def removeStopWords(data):\n",
    "    processed_data = []   \n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        for word in word_tokenize(text):\n",
    "            if word not in stopwords:\n",
    "                textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanText(x_train)\n",
    "cleanText(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Naive Bayes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.tolist()\n",
    "y_train = y_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.519910514541387\nF1 score =  0.513819664703217\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5185682326621924\nF1 score =  0.5209260908281389\n"
    }
   ],
   "source": [
    "lemmatize(x_train)\n",
    "lemmatize(x_test)\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5212527964205816\nF1 score =  0.5286343612334803\n"
    }
   ],
   "source": [
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1,stop_words=stopwords)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5592841163310962\nF1 score =  0.5248432223830198\n"
    }
   ],
   "source": [
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1,ngram_range=(2,2))) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.48322147651006714\nF1 score =  0.6486157590508063\n"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer()) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "classifier(GaussianNB(var_smoothing=1),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Try all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.4818791946308725\nF1 score =  0.6503623188405797\n"
    }
   ],
   "source": [
    "lemmatize(x_train)\n",
    "lemmatize(x_test)\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1,ngram_range=(2,2),stop_words=stopwords)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(var_smoothing=1),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(df,data):\n",
    "    listOfpos = []  \n",
    "    for text in data:\n",
    "        textWords =  word_tokenize(text)\n",
    "        listOfpos.append(nltk.pos_tag(textWords))\n",
    "\n",
    "    nounlist = []\n",
    "    verblist = []\n",
    "    adverblist = []\n",
    "    adjectivelist = []\n",
    "\n",
    "    for list1 in listOfpos:   \n",
    "        count=0 \n",
    "        noun=0\n",
    "        verb=0\n",
    "        adverb=0\n",
    "        adjective=0\n",
    "        for tuple1 in list1:\n",
    "            count+=1\n",
    "            verb += tuple1.count('VBD')\n",
    "            noun += tuple1.count('NN')\n",
    "            adverb += tuple1.count('VBG')\n",
    "            adjective  += tuple1.count('JJ') + tuple1.count('JJS')\n",
    "        if(count>0):\n",
    "            adverblist.append(adverb/count) # fractionAdverbs\n",
    "            verblist.append(verb/count) # fractionVerbs\n",
    "            adjectivelist.append(adjective/count) # fractionAdjectives\n",
    "            nounlist.append(noun/count) # fractionNouns \n",
    "        else:\n",
    "            adverblist.append(0) # fractionAdverbs\n",
    "            verblist.append(0) # fractionVerbs\n",
    "            adjectivelist.append(0) # fractionAdjectives\n",
    "            nounlist.append(0) # fractionNouns \n",
    "    df['fractionNouns'] = nounlist\n",
    "    df['fractionVerbs'] = verblist\n",
    "    df['fractionAdverbs'] = adverblist\n",
    "    df['fractionAdjectives'] = adjectivelist                    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "train_df = DataFrame(data=x_train)\n",
    "test_df  = DataFrame(data=x_test)\n",
    "\n",
    "pos_tag(train_df,x_train)\n",
    "pos_tag(test_df,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 1, expected 3947.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-207-85a3051665b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msp_train\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnouns_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msp_test\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnouns_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnouns_tfidf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnouns_tfidf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\construct.py\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \"\"\"\n\u001b[1;32m--> 465\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\construct.py\u001b[0m in \u001b[0;36mbmat\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    584\u001b[0m                                                     \u001b[0mexp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbrow_lengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m                                                     got=A.shape[0]))\n\u001b[1;32m--> 586\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbcol_lengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 1, expected 3947."
     ]
    }
   ],
   "source": [
    "nouns_train = train_df['fractionNouns']\n",
    "nouns_test = test_df['fractionNouns']\n",
    "nouns_tfidf_train,nouns_tfidf_test = vectorizeData(x_train,x_test,TfidfVectorizer(max_df=1.0,min_df=1))\n",
    "sp_train =  sp.coo_matrix(list(nouns_train))\n",
    "sp_test =  sp.coo_matrix(list(nouns_test))\n",
    "X_train = sp.hstack((nouns_tfidf_train, sp_train))\n",
    "X_test = sp.hstack((nouns_tfidf_test, sp_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3947\n"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_train = train_df['fractionVerbs']\n",
    "verbs_test = test_df['fractionVerbs']\n",
    "verbs_tfidf_train,verbs_tfidf_test = vectorizeData(verbs_train.astype('str'),verbs_test.astype('str'),TfidfVectorizer(max_df=1.0,min_df=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "adverbs_train = train_df['fractionAdverbs']\n",
    "adverbs_test = test_df['fractionAdverbs']\n",
    "adverbs_tfidf_train,adverbs_tfidf_test = vectorizeData(adverbs_train.astype('str'),adverbs_test.astype('str'),TfidfVectorizer(max_df=1.0,min_df=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives_train = train_df['fractionAdjectives']\n",
    "adjectives_test = test_df['fractionAdjectives']\n",
    "adjectives_tfidf_train,adjectives_tfidf_test = vectorizeData(adjectives_train.astype('str'),adjectives_test.astype('str'),TfidfVectorizer(max_df=1.0,min_df=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Random Forest Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5243847874720358\nF1 score =  0.06836108676599474\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier(RandomForestClassifier(),X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Support Vector Machine (SVM)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5217002237136465\nF1 score =  0.04808548530721282\n"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "classifier(svm.SVC(),X_train,y_train,X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}