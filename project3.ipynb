{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Mining - Project 3 - Spring Semester 2020\n",
    "---\n",
    "##  Iglezou Myrto  | sdi1700038\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import sys # only needed to determine Python version number\n",
    "import matplotlib # only needed to determine Matplotlib version number\n",
    "import nltk\n",
    "from pandas import DataFrame, read_csv\n",
    "from nltk import word_tokenize, BigramCollocationFinder\n",
    "from nltk.corpus import stopwords as stopwords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\train.csv\" \n",
    "testPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\impermium_verification_set.csv\" \n",
    "y_testPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\impermium_verification_labels.csv\"\n",
    "tempfile = pd.read_csv(trainPath,index_col=False)\n",
    "traindf = pd.DataFrame(data=tempfile)\n",
    "tempfile = pd.read_csv(testPath,index_col=False)\n",
    "testdf = pd.DataFrame(data=tempfile)\n",
    "tempfile = pd.read_csv(y_testPath,index_col=False)\n",
    "y_testdf = pd.DataFrame(data=tempfile)\n",
    "y_test = y_testdf[\"Insult\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from wordcloud import STOPWORDS,WordCloud\n",
    "\n",
    "punctuation = list(punctuation)\n",
    "punctuation.append('â€™')\n",
    "stopwordsPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\english_stopwords.txt\"\n",
    "file = open(stopwordsPath,\"r\",errors='ignore')\n",
    "stopwords_fromHub = [x.replace('\\n',\"\") for x in file.readlines()]\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords_nltk =  set(stopwords2.words('english'))\n",
    "stopwords.update(stopwords_nltk)\n",
    "stopwords.update(set(stopwords_fromHub))\n",
    "\n",
    "punctuation.append(\"``\")\n",
    "punctuation.append(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeData(X_train,X_test,vectorizer):\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def classifier(NB,x_train, y_train,x_test,y_test):\n",
    "    clf = NB\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    train_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    train_f1 = metrics.f1_score(y_test, y_pred)\n",
    "    print(\"Accuracy score = \",train_accuracy)\n",
    "    print(\"F1 score = \",train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "testdf['Comment'] = testdf['Comment'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "traindf['Comment'] = traindf['Comment'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "\n",
    "x_train = traindf['Comment']\n",
    "y_train = traindf['Insult']\n",
    "x_test = testdf['Comment']\n",
    "\n",
    "def cleanText(data):\n",
    "    processed_data = []   \n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        # text = re.sub(\"[^a-zA-Z]\",\" \",text) # matches everything from numbers and punctuations etc , leaving only the words\n",
    "        for word in word_tokenize(text):\n",
    "            word = word.lower()\n",
    "            if (word not in punctuation) and word.isalpha():\n",
    "                textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))\n",
    "\n",
    "def lemmatize(data):\n",
    "    processed_data = []   \n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        for word in word_tokenize(text):\n",
    "            word = WordNetLemmatizer().lemmatize(word)\n",
    "            textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))\n",
    "\n",
    "def removeStopWords(data):\n",
    "    processed_data = []   \n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        for word in word_tokenize(text):\n",
    "            if word not in stopwords:\n",
    "                textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanText(x_train)\n",
    "cleanText(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Naive Bayes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.tolist()\n",
    "y_train = y_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5221476510067115\nF1 score =  0.5291005291005291\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5257270693512305\nF1 score =  0.5411255411255411\n"
    }
   ],
   "source": [
    "lemmatize(x_train)\n",
    "lemmatize(x_test)\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5266219239373602\nF1 score =  0.5543386689132266\n"
    }
   ],
   "source": [
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1,stop_words=stopwords)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5570469798657718\nF1 score =  0.5240384615384615\n"
    }
   ],
   "source": [
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1,ngram_range=(2,2))) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.48322147651006714\nF1 score =  0.6479731789088692\n"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer()) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "classifier(GaussianNB(var_smoothing=1),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Try all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.4818791946308725\nF1 score =  0.6503623188405797\n"
    }
   ],
   "source": [
    "lemmatize(x_train)\n",
    "lemmatize(x_test)\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1,ngram_range=(2,2),stop_words=stopwords)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(var_smoothing=1),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Support Vector Machine (SVM)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.6662192393736018\nF1 score =  0.5174644243208278\n"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "tfidf_train,tfidf_test = vectorizeData(x_train,x_test,TfidfVectorizer(max_df=1.0,min_df=1))\n",
    "classifier(svm.SVC(),tfidf_train,y_train,tfidf_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.6469798657718121\nF1 score =  0.46868686868686865\n"
    }
   ],
   "source": [
    "tfidf_train,tfidf_test = vectorizeData(x_train,x_test,TfidfVectorizer(max_df=1.0,min_df=1,stop_words=stopwords))\n",
    "classifier(svm.SVC(),tfidf_train,y_train,tfidf_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfpos = []  \n",
    "for text in x_train:\n",
    "    textWords =  word_tokenize(text)\n",
    "    listOfpos.append(nltk.pos_tag(textWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun=0\n",
    "verb=0\n",
    "adverb=0\n",
    "adjective=0\n",
    "count=0\n",
    "for list1 in listOfpos:\n",
    "    for tuple1 in list1:\n",
    "        count+=1\n",
    "        if(tuple1[1]=='VBD'):\n",
    "            verb+=1\n",
    "        if(tuple1[1]=='NN'):\n",
    "            noun+=1\n",
    "        if(tuple1[1]=='VBG'):\n",
    "            adverb+=1\n",
    "        if(tuple1[1]=='JJ' or tuple1[1]=='JJS'):\n",
    "            adjective+=1\n",
    "train_fractionAdverbs  = adverb/count\n",
    "train_fractionVerbs = verb/count\n",
    "train_fractionAdjectives = adjective/count\n",
    "train_fractionNouns = noun/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "listOfpos = []  \n",
    "for text in x_test:\n",
    "    textWords =  word_tokenize(text)\n",
    "    listOfpos.append(nltk.pos_tag(textWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun=0\n",
    "verb=0\n",
    "adverb=0\n",
    "adjective=0\n",
    "count=0\n",
    "\n",
    "for list1 in listOfpos:    \n",
    "    for tuple1 in list1:\n",
    "        count+=1\n",
    "        verb += tuple1.count('VBD')\n",
    "        noun += tuple1.count('NN')\n",
    "        adverb += tuple1.count('VBG')\n",
    "        adjective  += tuple1.count('JJ') + tuple1.count('JJS')\n",
    "\n",
    "test_fractionAdverbs  = adverb/count\n",
    "test_fractionVerbs = verb/count\n",
    "test_fractionAdjectives = adjective/count\n",
    "test_fractionNouns = noun/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Random Forest Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.6335570469798658\nF1 score =  0.4347826086956523\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "tfidf_train,tfidf_test = vectorizeData(x_train,x_test,TfidfVectorizer(max_df=1.0,min_df=1))\n",
    "classifier(RandomForestClassifier(),tfidf_train,y_train,tfidf_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.6796420581655481\nF1 score =  0.5563816604708798\n"
    }
   ],
   "source": [
    "tfidf_train,tfidf_test = vectorizeData(x_train,x_test,TfidfVectorizer(max_df=1.0,min_df=1,stop_words=stopwords))\n",
    "classifier(RandomForestClassifier(),tfidf_train,y_train,tfidf_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.6389261744966444\nF1 score =  0.43368421052631584\n"
    }
   ],
   "source": [
    "tfidf_train,tfidf_test = vectorizeData(x_train,x_test,TfidfVectorizer(max_df=1.0,min_df=1,max_features=20000))\n",
    "classifier(RandomForestClassifier(),tfidf_train,y_train,tfidf_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "# Generate POS tags \n",
    "\n",
    "def pos_tag(df,data):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    pos_tags_column = []\n",
    "\n",
    "    for text in data:\n",
    "        pos_tags = []\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            pos_tags.append(token.pos_)\n",
    "        all_pos_tags = ' '.join(pos_tags)\n",
    "        pos_tags_column.append(all_pos_tags)\n",
    "        \n",
    "    df['Text_POS'] = pos_tags_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = DataFrame(data=x_train)\n",
    "test_df  = DataFrame(data=x_test)\n",
    "\n",
    "pos_tag(train_df,x_train)\n",
    "pos_tag(test_df,x_test)\n",
    "pos_train = train_df['Text_POS']\n",
    "pos_test = test_df['Text_POS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5324384787472036\nF1 score =  0.2572850035536603\n"
    }
   ],
   "source": [
    "pos_tfidf_vectorizer = TfidfVectorizer(max_df=1.0,min_df=1,stop_words=stopwords,smooth_idf=1)\n",
    "pos_tfidf_train = pos_tfidf_vectorizer.fit_transform(pos_train.astype('str'))\n",
    "pos_tfidf_test= pos_tfidf_vectorizer.transform(pos_test.astype('str'))\n",
    "\n",
    "classifier(RandomForestClassifier(),pos_tfidf_train,y_train,pos_tfidf_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5288590604026846\nF1 score =  0.08673026886383348\n"
    }
   ],
   "source": [
    "classifier(svm.SVC(),pos_tfidf_train,y_train,pos_tfidf_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}