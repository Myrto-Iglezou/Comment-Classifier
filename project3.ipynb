{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Mining - Project 3 - Spring Semester 2020\n",
    "---\n",
    "##  Iglezou Myrto  | sdi1700038\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import sys # only needed to determine Python version number\n",
    "import matplotlib # only needed to determine Matplotlib version number\n",
    "import nltk\n",
    "from pandas import DataFrame, read_csv\n",
    "from nltk import word_tokenize, BigramCollocationFinder\n",
    "from nltk.corpus import stopwords as stopwords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\train.csv\" \n",
    "testPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\impermium_verification_set.csv\" \n",
    "y_testPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\impermium_verification_labels.csv\"\n",
    "tempfile = pd.read_csv(trainPath,index_col=False)\n",
    "traindf = pd.DataFrame(data=tempfile)\n",
    "tempfile = pd.read_csv(testPath,index_col=False)\n",
    "testdf = pd.DataFrame(data=tempfile)\n",
    "tempfile = pd.read_csv(y_testPath,index_col=False)\n",
    "y_testdf = pd.DataFrame(data=tempfile)\n",
    "y_test = y_testdf[\"Insult\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from wordcloud import STOPWORDS,WordCloud\n",
    "\n",
    "punctuation = list(punctuation)\n",
    "punctuation.append('â€™')\n",
    "stopwordsPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\english_stopwords.txt\"\n",
    "file = open(stopwordsPath,\"r\",errors='ignore')\n",
    "stopwords_fromHub = [x.replace('\\n',\"\") for x in file.readlines()]\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords_nltk =  set(stopwords2.words('english'))\n",
    "stopwords.update(stopwords_nltk)\n",
    "stopwords.update(set(stopwords_fromHub))\n",
    "\n",
    "punctuation.append(\"``\")\n",
    "punctuation.append(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeData(X_train,X_test,vectorizer):\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(NB,x_train, y_train,x_test,y_test):\n",
    "    clf = NB\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    train_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    train_f1 = metrics.f1_score(y_test, y_pred)\n",
    "    print(\"Accuracy score = \",train_accuracy)\n",
    "    print(\"F1 score = \",train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "testdf['Comment'] = testdf['Comment'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "traindf['Comment'] = traindf['Comment'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "\n",
    "x_train = traindf['Comment']\n",
    "y_train = traindf['Insult']\n",
    "x_test = testdf['Comment']\n",
    "\n",
    "def cleanText(data):\n",
    "    processed_data = []   \n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        # text = re.sub(\"[^a-zA-Z]\",\" \",text) # matches everything from numbers and punctuations etc , leaving only the words\n",
    "        for word in word_tokenize(text):\n",
    "            word = word.lower()\n",
    "            if (word not in punctuation) and word.isalpha():\n",
    "                textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))\n",
    "\n",
    "def lemmatize(data):\n",
    "    processed_data = []   \n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        for word in word_tokenize(text):\n",
    "            word = WordNetLemmatizer().lemmatize(word)\n",
    "            textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))\n",
    "\n",
    "def removeStopWords(data):\n",
    "    processed_data = []   \n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        for word in word_tokenize(text):\n",
    "            if word not in stopwords:\n",
    "                textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanText(x_train)\n",
    "cleanText(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Naive Bayes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.tolist()\n",
    "y_train = y_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5221476510067115\nF1 score =  0.5291005291005291\n"
    }
   ],
   "source": [
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5257270693512305\nF1 score =  0.5415224913494809\n"
    }
   ],
   "source": [
    "lemmatize(x_train)\n",
    "lemmatize(x_test)\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5230425055928412\nF1 score =  0.5428816466552315\n"
    }
   ],
   "source": [
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1,stop_words=stopwords)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5615212527964206\nF1 score =  0.5351043643263756\n"
    }
   ],
   "source": [
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1,ngram_range=(2,2))) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.4823266219239374\nF1 score =  0.6471485208905153\n"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer()) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "classifier(GaussianNB(var_smoothing=1),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Try all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.4818791946308725\nF1 score =  0.6503623188405797\n"
    }
   ],
   "source": [
    "lemmatize(x_train)\n",
    "lemmatize(x_test)\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1,ngram_range=(2,2),stop_words=stopwords)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(var_smoothing=1),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Support Vector Machine (SVM)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.6662192393736018\nF1 score =  0.5174644243208278\n"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "tfidf_train,tfidf_test = vectorizeData(x_train,x_test,TfidfVectorizer(max_df=1.0,min_df=1))\n",
    "classifier(svm.SVC(),tfidf_train,y_train,tfidf_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.6469798657718121\nF1 score =  0.46868686868686865\n"
    }
   ],
   "source": [
    "tfidf_train,tfidf_test = vectorizeData(x_train,x_test,TfidfVectorizer(max_df=1.0,min_df=1,stop_words=stopwords))\n",
    "classifier(svm.SVC(),tfidf_train,y_train,tfidf_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfpos = []  \n",
    "for text in x_train:\n",
    "    textWords =  word_tokenize(text)\n",
    "    listOfpos.append(nltk.pos_tag(textWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun=0\n",
    "verb=0\n",
    "adverb=0\n",
    "adjective=0\n",
    "count=0\n",
    "for list1 in listOfpos:\n",
    "    for tuple1 in list1:\n",
    "        count+=1\n",
    "        if(tuple1[1]=='VBD'):\n",
    "            verb+=1\n",
    "        if(tuple1[1]=='NN'):\n",
    "            noun+=1\n",
    "        if(tuple1[1]=='VBG'):\n",
    "            adverb+=1\n",
    "        if(tuple1[1]=='JJ' or tuple1[1]=='JJS'):\n",
    "            adjective+=1\n",
    "train_fractionAdverbs  = adverb/count\n",
    "train_fractionVerbs = verb/count\n",
    "train_fractionAdjectives = adjective/count\n",
    "train_fractionNouns = noun/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "listOfpos = []  \n",
    "for text in x_test:\n",
    "    textWords =  word_tokenize(text)\n",
    "    listOfpos.append(nltk.pos_tag(textWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun=0\n",
    "verb=0\n",
    "adverb=0\n",
    "adjective=0\n",
    "count=0\n",
    "\n",
    "for list1 in listOfpos:    \n",
    "    for tuple1 in list1:\n",
    "        count+=1\n",
    "        verb += tuple1.count('VBD')\n",
    "        noun += tuple1.count('NN')\n",
    "        adverb += tuple1.count('VBG')\n",
    "        adjective  += tuple1.count('JJ') + tuple1.count('JJS')\n",
    "\n",
    "test_fractionAdverbs  = adverb/count\n",
    "test_fractionVerbs = verb/count\n",
    "test_fractionAdjectives = adjective/count\n",
    "test_fractionNouns = noun/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.21185007749753418\n"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Random Forest Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.6778523489932886\nF1 score =  0.5527950310559008\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "tfidf_train,tfidf_test = vectorizeData(x_train,x_test,TfidfVectorizer(max_df=1.0,min_df=1))\n",
    "classifier(RandomForestClassifier(),tfidf_train,y_train,tfidf_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.6796420581655481\nF1 score =  0.5496855345911951\n"
    }
   ],
   "source": [
    "tfidf_train,tfidf_test = vectorizeData(x_train,x_test,TfidfVectorizer(max_df=1.0,min_df=1,stop_words=stopwords))\n",
    "classifier(RandomForestClassifier(),tfidf_train,y_train,tfidf_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}