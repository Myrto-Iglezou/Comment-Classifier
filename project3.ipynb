{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Mining - Project 3 - Spring Semester 2020\n",
    "---\n",
    "##  Iglezou Myrto  | sdi1700038\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import sys # only needed to determine Python version number\n",
    "import matplotlib # only needed to determine Matplotlib version number\n",
    "import nltk\n",
    "from pandas import DataFrame, read_csv\n",
    "from nltk import word_tokenize, BigramCollocationFinder\n",
    "from nltk.corpus import stopwords as stopwords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\train.csv\" \n",
    "testPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\test.csv\" \n",
    "tempfile = pd.read_csv(trainPath,index_col=False)\n",
    "traindf = pd.DataFrame(data=tempfile)\n",
    "tempfile = pd.read_csv(testPath,index_col=False)\n",
    "testdf = pd.DataFrame(data=tempfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from wordcloud import STOPWORDS,WordCloud\n",
    "\n",
    "punctuation = list(punctuation)\n",
    "punctuation.append('â€™')\n",
    "stopwordsPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\english_stopwords.txt\"\n",
    "file = open(stopwordsPath,\"r\",errors='ignore')\n",
    "stopwords_fromHub = [x.replace('\\n',\"\") for x in file.readlines()]\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords_nltk =  set(stopwords2.words('english'))\n",
    "stopwords.update(stopwords_nltk)\n",
    "stopwords.update(set(stopwords_fromHub))\n",
    "\n",
    "punctuation.append(\"``\")\n",
    "punctuation.append(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def normalize_word(word):\n",
    "    # Will remove the encode token by token\n",
    "    word = str(word)\n",
    "    word = unicodedata.normalize('NFKD', word).encode('ASCII','ignore').decode('ASCII')\n",
    "    return word\n",
    "\n",
    "def textProcessor(inputData):\n",
    "    processed_data = []   \n",
    "    for text in inputData.values:\n",
    "        textWords = []\n",
    "        for word in word_tokenize(text):\n",
    "            # word = normalize_word(word)\n",
    "            word = word.lower()\n",
    "            word = WordNetLemmatizer().lemmatize(word)\n",
    "            word.replace('\\n',' ')\n",
    "            word.replace('<html>','')\n",
    "            word.replace('<p>','')\n",
    "            word.replace(\"'s\",'')\n",
    "            if word.isnumeric():\n",
    "                word = '#num#'\n",
    "            if (word not in stopwords) and (word not in punctuation) and word.isalpha():\n",
    "                textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "        \n",
    "    inputData.update(pd.Series(processed_data,index=inputData.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Naive Bayes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "testdf['Comment'] = testdf['Comment'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "traindf['Comment'] = traindf['Comment'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "\n",
    "train = traindf['Comment']\n",
    "test = testdf['Comment']\n",
    "\n",
    "textProcessor(train)\n",
    "textProcessor(test)\n",
    "\n",
    "countVectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "count_train = countVectorizer.fit_transform(train)\n",
    "count_test = countVectorizer.fit_transform(test)\n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "source": [
    "# - Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}