{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Mining - Project 3 - Spring Semester 2020\n",
    "---\n",
    "##  Iglezou Myrto  | sdi1700038\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import sys # only needed to determine Python version number\n",
    "import matplotlib # only needed to determine Matplotlib version number\n",
    "import nltk\n",
    "from pandas import DataFrame, read_csv\n",
    "from nltk import word_tokenize, BigramCollocationFinder\n",
    "from nltk.corpus import stopwords as stopwords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\train.csv\" \n",
    "testPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\impermium_verification_set.csv\" \n",
    "y_testPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\impermium_verification_labels.csv\"\n",
    "tempfile = pd.read_csv(trainPath,index_col=False)\n",
    "traindf = pd.DataFrame(data=tempfile)\n",
    "tempfile = pd.read_csv(testPath,index_col=False)\n",
    "testdf = pd.DataFrame(data=tempfile)\n",
    "tempfile = pd.read_csv(y_testPath,index_col=False)\n",
    "y_testdf = pd.DataFrame(data=tempfile)\n",
    "y_test = y_testdf[\"Insult\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from wordcloud import STOPWORDS,WordCloud\n",
    "\n",
    "punctuation = list(punctuation)\n",
    "punctuation.append('â€™')\n",
    "stopwordsPath = r\"C:\\Users\\myrto\\Documents\\GitHub\\-Project-3-DataMining\\english_stopwords.txt\"\n",
    "file = open(stopwordsPath,\"r\",errors='ignore')\n",
    "stopwords_fromHub = [x.replace('\\n',\"\") for x in file.readlines()]\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords_nltk =  set(stopwords2.words('english'))\n",
    "stopwords.update(stopwords_nltk)\n",
    "stopwords.update(set(stopwords_fromHub))\n",
    "\n",
    "punctuation.append(\"``\")\n",
    "punctuation.append(\"...\")\n",
    "punctuation.append('\"\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeData(X_train,X_test,vectorizer):\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def classifier(NB,x_train, y_train,x_test,y_test):\n",
    "    clf = NB\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    train_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    train_f1 = metrics.f1_score(y_test, y_pred)\n",
    "    print(\"Accuracy score = \",train_accuracy)\n",
    "    print(\"F1 score = \",train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "testdf['Comment'] = testdf['Comment'].replace(\"\\n\",\" \")\n",
    "traindf['Comment'] = traindf['Comment'].replace(\"\\n\",\" \")\n",
    "\n",
    "testdf['Comment'] = testdf['Comment'].str.replace('http\\S+|www.\\S+', \" \", case=False)\n",
    "traindf['Comment'] = traindf['Comment'].str.replace('http\\S+|www.\\S+', \" \", case=False)\n",
    "\n",
    "x_train = traindf['Comment']\n",
    "y_train = traindf['Insult']\n",
    "x_test = testdf['Comment']\n",
    "\n",
    "def cleanText(data):\n",
    "    processed_data = []   \n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        text = re.sub(\"[^a-zA-Z]\",\" \",text) # matches everything from numbers and punctuations etc , leaving only the words\n",
    "        for word in word_tokenize(text):\n",
    "            word = word.lower()\n",
    "            if (word not in punctuation) and word.isalpha():\n",
    "                textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))\n",
    "\n",
    "def lemmatize(data):\n",
    "    processed_data = []   \n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        for word in word_tokenize(text):\n",
    "            word = WordNetLemmatizer().lemmatize(word)\n",
    "            textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))\n",
    "\n",
    "def removeStopWords(data):\n",
    "    processed_data = []   \n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        for word in word_tokenize(text):\n",
    "            if word not in stopwords:\n",
    "                textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))\n",
    "\n",
    "def stemWords(data):\n",
    "    processed_data = []   \n",
    "    porter = PorterStemmer()\n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        for word in word_tokenize(text):\n",
    "            word = porter.stem(word)\n",
    "            textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))\n",
    "\n",
    "def FixWords(data):\n",
    "    processed_data = []\n",
    "    for text in data.values:\n",
    "        cleaned = ''.join(''.join(s)[:1] for _, s in itertools.groupby(text))\n",
    "        processed_data.append(cleaned)\n",
    "    data.update(pd.Series(processed_data,index=data.index))     \n",
    "\n",
    "def fixSlangWords(data):\n",
    "    processed_data = []   \n",
    "    for text in data.values:\n",
    "        textWords = []\n",
    "        for word in word_tokenize(text):\n",
    "            word = _slang_loopup(word)\n",
    "            textWords.append(word)\n",
    "        processed_data.append(' '.join(textWords))\n",
    "    data.update(pd.Series(processed_data,index=data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.to_csv(\"out1.txt\",index=False)\n",
    "cleanText(x_train)\n",
    "cleanText(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removeStopWords(x_train)\n",
    "# x_train.to_csv(\"out.txt\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Naive Bayes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.tolist()\n",
    "y_train = y_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.519910514541387\nF1 score =  0.513819664703217\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5185682326621924\nF1 score =  0.5209260908281389\n"
    }
   ],
   "source": [
    "lemmatize(x_train)\n",
    "lemmatize(x_test)\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5212527964205816\nF1 score =  0.5286343612334803\n"
    }
   ],
   "source": [
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1,stop_words=stopwords)) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5592841163310962\nF1 score =  0.5248432223830198\n"
    }
   ],
   "source": [
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1,ngram_range=(2,2))) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(GaussianNB(),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.6796420581655481\nF1 score =  0.6324435318275154\n"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer()) \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "classifier(MultinomialNB(alpha=1),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Try all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.5682326621923938\nF1 score =  0.2571208622016936\n"
    }
   ],
   "source": [
    "lemmatize(x_train)  \n",
    "lemmatize(x_test)\n",
    "\n",
    "count_train,count_test = vectorizeData(x_train,x_test,CountVectorizer(max_df=1.0, min_df=1,ngram_range=(2,2),stop_words=stopwords))  \n",
    "\n",
    "count_train = count_train.toarray()\n",
    "count_test = count_test.toarray()\n",
    "\n",
    "classifier(MultinomialNB(alpha=1),count_train, y_train,count_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(df,data):\n",
    "    listOfpos = []  \n",
    "    for text in data:\n",
    "        textWords =  word_tokenize(text)\n",
    "        listOfpos.append(nltk.pos_tag(textWords))\n",
    "\n",
    "    nounlist = []\n",
    "    verblist = []\n",
    "    adverblist = []\n",
    "    adjectivelist = []\n",
    "\n",
    "    for list1 in listOfpos:   \n",
    "        count=0 \n",
    "        noun=0\n",
    "        verb=0\n",
    "        adverb=0\n",
    "        adjective=0\n",
    "        for tuple1 in list1:\n",
    "            count+=1\n",
    "            verb += tuple1.count('VBD')\n",
    "            noun += tuple1.count('NN')\n",
    "            adverb += tuple1.count('VBG')\n",
    "            adjective  += tuple1.count('JJ') + tuple1.count('JJS')\n",
    "        if(count>0):\n",
    "            adverblist.append(adverb/count) # fractionAdverbs\n",
    "            verblist.append(verb/count) # fractionVerbs\n",
    "            adjectivelist.append(adjective/count) # fractionAdjectives\n",
    "            nounlist.append(noun/count) # fractionNouns \n",
    "        else:\n",
    "            adverblist.append(0) # fractionAdverbs\n",
    "            verblist.append(0) # fractionVerbs\n",
    "            adjectivelist.append(0) # fractionAdjectives\n",
    "            nounlist.append(0) # fractionNouns \n",
    "    df['fractionNouns'] = nounlist\n",
    "    df['fractionVerbs'] = verblist\n",
    "    df['fractionAdverbs'] = adverblist\n",
    "    df['fractionAdjectives'] = adjectivelist                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "lemmatize(x_train)  # 1st step to improve score - lemmatization\n",
    "lemmatize(x_test)\n",
    "\n",
    "stemWords(x_train)  # 4th step to improve score - stemming\n",
    "stemWords(x_test)\n",
    "\n",
    "# FixWords(x_train)\n",
    "# FixWords(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(x_train).split()).value_counts()[:10] # 5th step to improve score - Common word removal\n",
    "freq = list(freq.index)\n",
    "x_train = x_train.apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "freq = pd.Series(' '.join(x_test).split()).value_counts()[:10] # 5th step to improve score - Common word removal\n",
    "freq = list(freq.index)\n",
    "x_test = x_test.apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(x_train).split()).value_counts()[-10:] # 5th step to improve score - Rare word removal\n",
    "freq = list(freq.index)\n",
    "x_train = x_train.apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "freq = pd.Series(' '.join(x_test).split()).value_counts()[-10:] # 5th step to improve score - Rare word removal\n",
    "freq = list(freq.index)\n",
    "x_test = x_test.apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import TextBlob\n",
    "# x_train.apply(lambda x: str(TextBlob(x).correct()))\n",
    "# x_test.apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = DataFrame(data=x_train)\n",
    "test_df  = DataFrame(data=x_test)\n",
    "pos_tag(train_df,x_train)\n",
    "pos_tag(test_df,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "tfidf_train,tfidf_test = vectorizeData(x_train,x_test,TfidfVectorizer(max_df=1.0, min_df=1,stop_words=stopwords)) \n",
    "# 2nd step to improve score - remove stopwords\n",
    "\n",
    "tfidf_train = tfidf_train.toarray()\n",
    "tfidf_test = tfidf_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_train = train_df['fractionNouns']\n",
    "nouns_test = test_df['fractionNouns']\n",
    "\n",
    "nouns_train = np.asarray(nouns_train)\n",
    "nouns_train = np.vstack(nouns_train)\n",
    "nouns_test = np.asarray(nouns_test)\n",
    "nouns_test = np.vstack(nouns_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_train = train_df['fractionVerbs']\n",
    "verbs_test = test_df['fractionVerbs']\n",
    "\n",
    "verbs_train = np.asarray(verbs_train)\n",
    "verbs_train = np.vstack(verbs_train)\n",
    "verbs_test = np.asarray(verbs_test)\n",
    "verbs_test = np.vstack(verbs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "adverbs_train = train_df['fractionAdverbs']\n",
    "adverbs_test = test_df['fractionAdverbs']\n",
    "\n",
    "adverbs_train = np.asarray(adverbs_train)\n",
    "adverbs_train = np.vstack(adverbs_train)\n",
    "adverbs_test = np.asarray(adverbs_test)\n",
    "adverbs_test = np.vstack(adverbs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives_train = train_df['fractionAdjectives']\n",
    "adjectives_test = test_df['fractionAdjectives']\n",
    "\n",
    "adjectives_train = np.asarray(adjectives_train)\n",
    "adjectives_train = np.vstack(adjectives_train)\n",
    "adjectives_test = np.asarray(adjectives_test)\n",
    "adjectives_test = np.vstack(adjectives_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = np.hstack((nouns_train,verbs_train,adverbs_train,adjectives_train,tfidf_train))\n",
    "test_final = np.hstack((nouns_test,verbs_test,adverbs_test,adjectives_test,tfidf_test))\n",
    "\n",
    "X_train = sp.csr_matrix(train_final)\n",
    "X_test = sp.csr_matrix(test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Random Forest Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.668903803131991\nF1 score =  0.528061224489796\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier(RandomForestClassifier(n_estimators=100,criterion='gini'),X_train,y_train,X_test,y_test) # 3rd step to improve score - fix parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Support Vector Machine (SVM)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy score =  0.6993288590604027\nF1 score =  0.5971223021582733\n"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "classifier(svm.SVC(C=1,kernel='linear',gamma=1),X_train,y_train,X_test,y_test) # 3rd step to improve score - fix parameters"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}